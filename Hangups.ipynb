{
 "metadata": {
  "name": "",
  "signature": "sha256:fa4d3c6d1564c90459f3ba3952679e1e634a155901f9f5c82285355c45e548d6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Hangups Open Source News Page Sentiment Tracker"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hangups is an open source tool designed in ipython notebook for tracking and evaluating the sentiments of local newspaper commenters. This was intended as a tool for social research and to answer the question 'are the people in my area more prejudiced than elsewhere'. Hangups does not store data or track individual commenters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hangups requires a facebook dev account and application key, ipython notebook, python 2.7, and martey's excellent python facebook-sdk from https://github.com/pythonforfacebook/facebook-sdk."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To access the facebook api, you will need to create a login file 'hangups.cfg' in the same directory with the following format:\n",
      "<br><br>application = [appID]\n",
      "<br>secret = [appSecret]\n",
      "<br>token = [token]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Libraries & Dependancies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import facebook\n",
      "import datetime\n",
      "import requests\n",
      "\n",
      "from time import sleep\n",
      "from copy import deepcopy\n",
      "from dateutil import parser, tz\n",
      "from random import sample\n",
      "from math import sqrt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfgFile = 'hangups.cfg'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Loader Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load cfg from file\n",
      "def getCfg(fileRef):\n",
      "    params = dict()\n",
      "    inFile = open(fileRef)\n",
      "    content = inFile.readlines()\n",
      "    inFile.close()\n",
      "    for line in content:\n",
      "        splitLine = line.replace(' ','').replace('\\n','').split('=')\n",
      "        params[splitLine[0]] = splitLine[1]\n",
      "    return params\n",
      "\n",
      "\n",
      "#Pull data via facebook API\n",
      "def getData(pageRef,login,field='posts',pages = 5):\n",
      "    if type(pageRef) is not list:\n",
      "        pageRef = [pageRef]\n",
      "    data = dict(); temp = []\n",
      "    for entry in pageRef:\n",
      "        count = 0\n",
      "        graph = facebook.GraphAPI(login['token'])\n",
      "        posts = graph.get_connections(entry,field)\n",
      "        temp = posts['data']\n",
      "        while count < pages-1 and True:\n",
      "            try:\n",
      "                posts = requests.get(posts['paging']['next']).json()\n",
      "                temp += posts['data']\n",
      "            except:\n",
      "                break\n",
      "            count += 1\n",
      "        data[entry] = temp\n",
      "        print \"Retrieved %s pages for site %s\" % (count+1,entry)\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Format & Cleanup Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Returns true if one ore more words present in text\n",
      "def hasWord(text,words):\n",
      "    return len([entry for entry in words if entry.lower() in text.lower()]) != 0\n",
      "\n",
      "\n",
      "#Filters a dictionary for entries where presence of words = state(True/False) in a given field\n",
      "def filterDict(dictionary,field,words,state):\n",
      "    return dict([(key,entry) for key,entry in dictionary.iteritems() if hasWord(entry[field],words) == state])\n",
      "\n",
      "\n",
      "#Filters a list for entries where presence of words = state(True/False)\n",
      "def filterList(listed,words,state):\n",
      "    return [entry for entry in listed if hasWord(entry,words) == state]\n",
      "\n",
      "\n",
      "#Returns a string formatted date in local time\n",
      "def formatDate(date, setLocal = False):\n",
      "    if type(date) is str or type(date) is unicode:\n",
      "        date = parser.parse(date)\n",
      "    if setLocal:\n",
      "        utcZone = tz.tzutc()\n",
      "        localZone = tz.tzlocal()\n",
      "        date.replace(tzinfo=utcZone)\n",
      "        date = date.astimezone(localZone)\n",
      "    return date.strftime(\"%B %d, %Y %H:%M\")\n",
      "\n",
      "\n",
      "#Pulls data by key if available, else returns default\n",
      "def bestShot(dataIn,key,default):\n",
      "    try:\n",
      "        temp = dataIn[key]\n",
      "    except:\n",
      "        temp = default\n",
      "    return temp\n",
      "\n",
      "\n",
      "#Creates comments data structure for analysis\n",
      "def getComments(dataIn):\n",
      "    data = dict()\n",
      "    for entry in dataIn:\n",
      "        try: \n",
      "            comments = [comment['message'] for comment in entry['comments']['data']]\n",
      "        except:\n",
      "            comments = []\n",
      "\n",
      "        item = {'name':bestShot(entry,'name','null'),\n",
      "                'message':bestShot(entry,'message','null'),\n",
      "                'comments':comments,\n",
      "                'likes':len(bestShot(bestShot(entry,'likes',{}),'data',[])),\n",
      "                'link':bestShot(entry,'link','null')}\n",
      "        data[entry['id']] = item\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Analysis Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#Filters data set into postitive and negative sentiment subsets\n",
      "def checkSentiment(dataIn,words):\n",
      "    commentsPos = []\n",
      "    commentsNeg = []\n",
      "    for item in dataIn.values():\n",
      "        commentsPos += filterList(item['comments'],words,True)\n",
      "        commentsNeg += filterList(item['comments'],words,False)\n",
      "    return {'pos':commentsPos,\n",
      "            'neg':commentsNeg}\n",
      "\n",
      "\n",
      "#Prepares data for A vs B comparison\n",
      "def getAvB(dataInA,dataInB,nameA,nameB,sentimentKeys):\n",
      "    return compareTwo(dataInA,dataInB,nameA,nameB,sentimentKeys)\n",
      "\n",
      "\n",
      "#Prepares data for Topic True vs False comparison\n",
      "def get2v2(dataIn,topicKeys,sentimentKeys):\n",
      "    withKeyword = filterDict(dataIn,'message',topicKeys,True)\n",
      "    withoutKeyword = filterDict(dataIn,'message',topicKeys,False)\n",
      "    return compareTwo(withKeyword,withoutKeyword,'positive','negative',sentimentKeys)\n",
      "\n",
      "\n",
      "#Shows article title samples and generates separated data by topic and sentiment for comparison\n",
      "def compareTwo(dataA,dataB,nameA,nameB,sentimentKeys):\n",
      "    aTitles = [x['name'] for x in [entry for entry in dataA.values() if 'name' in entry.keys() and entry['name'] != 'null']]\n",
      "    bTitles = [x['name'] for x in [entry for entry in dataB.values() if 'name' in entry.keys() and entry['name'] != 'null']]\n",
      "    totalKa = len(dataA); totalKb = len(dataB)\n",
      "    totalKat = len(aTitles); totalKbt = len(bTitles)\n",
      "    n = int(min(5,totalKat)); m = int(min(5,totalKbt))\n",
      "    \n",
      "    print \"%s entries found for topic %s and %s entries found for topic %s\" % (totalKa,\n",
      "                                                                               nameA,\n",
      "                                                                               totalKb,\n",
      "                                                                               nameB)\n",
      "    print nameA,\"article titles:\"\n",
      "    print '\\n'.join(['\\t\"'+entry+'\"' for entry in sample(aTitles,n)])\n",
      "    print nameB,\"article titles:\"\n",
      "    print '\\n'.join(['\\t\"'+entry+'\"' for entry in sample(bTitles,m)])\n",
      "    print \n",
      "    \n",
      "    sortedA = checkSentiment(dataA,sentimentKeys)\n",
      "    sortedB = checkSentiment(dataB,sentimentKeys)\n",
      "    \n",
      "    if nameA == 'positive':\n",
      "        comments = {'T+S+':sortedA['pos'],\n",
      "                    'T+S-':sortedA['neg'],\n",
      "                    'T-S+':sortedB['pos'],\n",
      "                    'T-S-':sortedB['neg']}\n",
      "    else:\n",
      "        comments = {'A+S+':sortedA['pos'],\n",
      "                    'A+S-':sortedA['neg'],\n",
      "                    'B+S+':sortedB['pos'],\n",
      "                    'B+S-':sortedB['neg']}\n",
      "        \n",
      "    counts = dict([(key,float(len(item))) for key,item in comments.iteritems()])\n",
      "    \n",
      "    return {'comments':comments,\n",
      "            'counts':counts}\n",
      "\n",
      "\n",
      "#Describes sorted data by odds ratio and counts, shows subset post samples\n",
      "def describeComparison(comparison,show = 3,nameA = '',nameB = ''):\n",
      "    counts = comparison['counts']\n",
      "    comments = comparison['comments']\n",
      "    for key,item in counts.iteritems():\n",
      "        if 'T+' in key:\n",
      "            topicPos = True\n",
      "        elif 'T-' in key:\n",
      "            topicPos = False\n",
      "        elif 'A+' in key:\n",
      "            topicPos = nameA\n",
      "        elif 'B+' in key:\n",
      "            topicPos = nameB\n",
      "        sentPos = 'S+' in key\n",
      "        print \"%s posts were found for topic=%s, sentiment=%s\" % (int(item),topicPos,sentPos)\n",
      "        n = int(min(show,item))\n",
      "        toShow = '\\n'.join(['\\t\"'+entry+'\"' for entry in sample(comments[key],n)])\n",
      "        print \"Random comment sample:\"\n",
      "        print toShow+'\\n'\n",
      "    \n",
      "    try:\n",
      "        if type(topicPos) is bool:\n",
      "            OddsRatio = (counts['T+S+']/counts['T+S-'])/(counts['T-S+']/counts['T-S-'])\n",
      "            subjectA = 'Topic Positive'; subjectB = 'Topic Negative'\n",
      "        else:\n",
      "            OddsRatio = (counts['A+S+']/counts['A+S-'])/(counts['B+S+']/counts['B+S-'])\n",
      "            subjectA = 'Page ' + nameA; subjectB = 'Page ' + nameB\n",
      "    except:\n",
      "        OddsRatio = 'NaN'\n",
      "    try:\n",
      "        StandardError = sqrt(sum([1./entry for entry in counts.values()]))\n",
      "    except:\n",
      "        StandardError = 'NaN'\n",
      "    oddsOut = str(OddsRatio)[0:5]\n",
      "    errOut = str(StandardError)[0:5]\n",
      "    print \"The odds ratio for being found sentiment positive given topic %s vs topic %s is %s with a standard error of %s.\\n\" % (subjectA,\n",
      "                                                                                                                                 subjectB,\n",
      "                                                                                                                                 oddsOut,\n",
      "                                                                                                                                 errOut)\n",
      "                \n",
      "#Initial data preparation & comment extraction step, can exclude articles not posted by page itself if 'filterAuthor' set to True\n",
      "def prepOne(dataIn,filterAuthor = False,title = 'null'):\n",
      "    pageA = dataIn.keys()[0]; dataA = dataIn[pageA]\n",
      "    print \"Prepping data for facebook page '%s' on %s.\" % (pageA,formatDate(datetime.datetime.now()))\n",
      "    \n",
      "    if filterAuthor:\n",
      "        byPageA = [item for item in dataA.values() if item['from']['name'].lower() == page]\n",
      "    else:\n",
      "        byPageA = dataA\n",
      "        \n",
      "    withCommentsA = [item for item in byPageA if 'comments' in item]\n",
      "    originalNA = len(byPageA); entriesNA = len(dataA); commentsNA = len(withCommentsA)\n",
      "    newestTimeA = formatDate(dataA[0]['created_time'],setLocal=True)\n",
      "    oldestTimeA = formatDate(dataA[-1]['created_time'],setLocal=True)\n",
      "    \n",
      "    print \"%s: %s total entries found with %s original posts, %s of which have recieved comments.\" % (pageA,\n",
      "                                                                                                   entriesNA,\n",
      "                                                                                                   originalNA,\n",
      "                                                                                                   commentsNA)\n",
      "    print \"Posts were generated between %s and %s\\n\" % (newestTimeA,oldestTimeA)\n",
      "    return getComments(withCommentsA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Compares one page to another by exclusions and sentiment\n",
      "def comparisonReport(dataA,dataB,exclusions,tracked,exclusionDirection=False,filterAuthor=False):\n",
      "        pageA = dataA.keys()[0]; pageB = dataB.keys()[0]\n",
      "        commentedA = prepOne(dataA,filterAuthor=filterAuthor)\n",
      "        commentedB = prepOne(dataB,filterAuthor=filterAuthor) \n",
      "        postExclusionA = filterDict(commentedA,'message',exclusions,exclusionDirection)\n",
      "        postExclusionB = filterDict(commentedB,'message',exclusions,exclusionDirection)\n",
      "        print \"Page %s: %s entries removed by exclusion keywords %s.\" % (pageA,len(commentedA)-len(postExclusionA),exclusions)\n",
      "        print \"Page %s: %s entries removed by exclusion keywords %s.\" % (pageB,len(commentedB)-len(postExclusionB),exclusions)\n",
      "        \n",
      "        for subject,tracker in tracked.iteritems():\n",
      "            print \"\\nAnalyzing incidence of comment sentiment keywords vs title topic keywords for subject %s.\" % subject\n",
      "            print \"Sentiment keywords:\\t%s\" % tracker\n",
      "            comparison = getAvB(postExclusionA,postExclusionB,pageA,pageB,tracker)\n",
      "            describeComparison(comparison,nameA=pageA,nameB=pageB)\n",
      "            print\n",
      "            \n",
      "            \n",
      "#Compares articles with and without a given topic to each other by exclusions and sentiment            \n",
      "def generateReport(dataIn,keywords,exclusions,tracked,exclusionDirection=False,filterAuthor=False):\n",
      "    for page, data in dataIn.iteritems():\n",
      "        commented = prepOne({page:data},filterAuthor=filterAuthor)\n",
      "        postExclusion = filterDict(commented,'message',exclusions,exclusionDirection)\n",
      "        print \"%s entries removed by exclusion keywords %s.\" % (len(commented)-len(postExclusion),exclusions)\n",
      "        \n",
      "        for subject,tracker in tracked.iteritems():\n",
      "            print \"\\nAnalyzing incidence of comment sentiment keywords vs title topic keywords for subject %s.\" % subject\n",
      "            print \"Sentiment keywords:\\t%s\" % tracker\n",
      "            print \"Topic keywords:\\t\\t%s\" % keywords\n",
      "            comparison = get2v2(postExclusion,keywords,tracker)\n",
      "            describeComparison(comparison)\n",
      "            print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Loading (do this once, may use multiple pages)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "login = getCfg(cfgFile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Max is 250 pages (each with many posts), will stop after limit is reached\n",
      "#Allow some time before loading additional sets as rate limits may limit results\n",
      "wdbj7Data = getData('wdbj7',login,pages=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Retrieved 100 pages for site wdbj7\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nytimesData = getData('nytimes',login,pages=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Retrieved 100 pages for site nytimes\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 1a: The words 'hang him/her/them' come up disturbingly often in the local law enforcement related facebook posts, as well as some general bloodthirst. How strongly do bloodthirst sentiments tie in to articles of crime related police activity? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Words in and article title that will cause the article to be included\n",
      "criminalJusticeWords = ['police','sheriff','deputies','deputy','indict','arrest','charged','convicted','sentenced','prison','jail']\n",
      "\n",
      "#Words in an article title that will cause the article to be excluded\n",
      "articleExclusions = []\n",
      "\n",
      "#Usage: 1 or more topics given as a dictionary where keys = topic names and values = list of sentiment keywords\n",
      "#Report will analyze each sentiment topic\n",
      "sentimentWords = {'fascism':['hang him','hang her','hang them','burn him','burn her']}\n",
      "\n",
      "#Usage: generateReport(data set name,\n",
      "#                        topic keywords list,\n",
      "#                        exclusion keyword list,\n",
      "#                        sentiment keywords dictionary)\n",
      "generateReport(wdbj7Data,criminalJusticeWords,articleExclusions,sentimentWords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 1b: Beyond law enforcement, how often do these phrases come up in general posts on WDBJ7 compared to the NY Times? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Words in an article title that will cause the article to be excluded\n",
      "articleExclusions = []\n",
      "\n",
      "#Usage: 1 or more topics given as a dictionary where keys = topic names and values = list of sentiment keywords\n",
      "#Report will analyze each sentiment topic\n",
      "sentimentWords = {'fascism':['hang him','hang her','hang them']}\n",
      "\n",
      "#Usage: comparisonReport(data set A name,\n",
      "#                        data set B name,\n",
      "#                        exclusion keyword list,\n",
      "#                        sentiment keywords dictionary)\n",
      "comparisonReport(wdbj7Data,nytimesData,articleExclusions,sentimentWords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 2: How important is walmart in the discussion of minimum wage?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topicWords = ['walmart','mcdonalds']\n",
      "articleExclusions = []\n",
      "sentimentWords = {'should probably pay people more':['minimum wage']}\n",
      "generateReport(wdbj7Data,topicWords,articleExclusions,sentimentWords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 3: How important are vaccines in the discussion of autism?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topicWords = ['vaccine','vaccination','flu shot']\n",
      "articleExclusions = []\n",
      "sentimentWords = {'antivaccers':['autism']}\n",
      "generateReport(dataDump,topicWords,articleExclusions,sentimentWords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
